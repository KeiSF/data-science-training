{
 "metadata": {
  "name": "",
  "signature": "sha256:684d386045fd2306dd3cb343296c8510bc428c04548880d89bafe68ce827c19c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "############## IMPORTS ###################\n",
      "import sys\n",
      "sys.path.insert(0, '../mylib/')\n",
      "from data_load import load_csv_training\n",
      "from data_load import load_csv_test\n",
      "from sklearn import linear_model\n",
      "from sklearn import svm\n",
      "from sklearn import naive_bayes\n",
      "from sklearn import neighbors\n",
      "from sklearn import tree\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn import cross_validation\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.ensemble import AdaBoostClassifier\n",
      "from sklearn.lda import LDA\n",
      "from sklearn.feature_selection import SelectKBest\n",
      "from sklearn.feature_selection import f_classif\n",
      "from sklearn.feature_selection import VarianceThreshold\n",
      "from sklearn.feature_selection import RFE\n",
      "from sklearn.feature_selection import RFECV\n",
      "from numpy import *\n",
      "import pandas as pd\n",
      "import seaborn as sns\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "############## FILES ###################\n",
      "output=open('output','wb')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#####################\n",
      "### DATA LOADING\n",
      "#####################\n",
      "\n",
      "#features,labels=load_csv_training('train.csv',',')\n",
      "\n",
      "training_set=pd.read_csv('train.csv',index_col=0)\n",
      "\n",
      "# [TEST] the data loading by writing arrays in file:\n",
      "\n",
      "# counter=0\n",
      "# for new_datapoint in features:\n",
      "# \tfor feature in new_datapoint:\n",
      "# \t\toutput.write(str(feature)+\" \")\n",
      "# \toutput.write(' ')\t\n",
      "# \toutput.write(str(labels[counter]))\n",
      "# \toutput.write(\"\\n\")\n",
      "# \tcounter=counter+1\n",
      "\n",
      "\n",
      "# From now onwards: \n",
      "\t# features is a list containing lists of datapoint features\n",
      "\t# labels is a list of labels\n",
      "# This structures can be used as inputs for the .fit() function of the learners\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#################################\n",
      "### DATA VISUALIZATION (SEABORN)\n",
      "#################################\n",
      "\n",
      "%matplotlib tk\n",
      "\n",
      "# Create a plot with cover type vs any other variable\n",
      "#sns.regplot(\"Elevation\", \"Cover_Type\", training_set);\n",
      "\n",
      "#print training_set.ix[:,0:3]\n",
      "\n",
      "# Create a scatter plot (regplot()) \n",
      "#sns.pairplot(training_set.ix[:,50:55], size=5, diag_kind='kde') # it fails when using a larger amount of features\n",
      "#g = sns.PairGrid(training_set)\n",
      "#g.map(plt.scatter)\n",
      "\n",
      "\n",
      "# Create a correlation plot (corrplot())\n",
      "\n",
      "# Show an histogram for th features\n",
      "#training_set.hist()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###################################################\n",
      "# extract features and labels from training_set (): \n",
      "###################################################\n",
      "training_set_array = array(training_set)\n",
      "features = training_set_array[0:,0:-1]\n",
      "labels = ravel(training_set_array[0:,-1:]) # Can we actually do this? It may use a lot of memory for big data sets\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "######################\n",
      "### FEATURE SELECTION\n",
      "######################\n",
      "\n",
      "#print len(features)\n",
      "\n",
      "# Variance threshold\n",
      "#sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
      "#features = sel.fit_transform(features)\n",
      "\n",
      "# Univariate feature selection\n",
      "#features = SelectKBest(f_classif, k=2).fit_transform(features, labels)\n",
      "\n",
      "# Use Linear Discriminant Analysis (LDA)\n",
      "#lda = LDA() # lda = LDA(n_components=5) 1..5 (Number of components (< n_classes - 1) for dimensionality reduction)\n",
      "#features= lda.fit(features, labels).transform(features)\n",
      "\n",
      "# Use Principal Component Analysis (PCA)\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "############################\n",
      "### COMMENT FOR SUBMISSION:\n",
      "###############################################\n",
      "### LEARNING AND TESTING WITH CROSS-VALIDATION\n",
      "###############################################\n",
      "###############################################\n",
      "print \"\"\n",
      "\n",
      "features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(features,labels, test_size=0.4, random_state=0)\n",
      "\n",
      "# [TEST] the splits\n",
      "# for list_a in a:\n",
      "# \tfor elem in list_a:\n",
      "# \t\toutput.write(str(elem))\n",
      "# \t\toutput.write(\" \")\n",
      "# \toutput.write(\"\\n\\n\\n\\n\\n\\n\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Logistic Regression\n",
      "estimator = linear_model.LogisticRegression()\n",
      "clf = estimator.fit(features_train, labels_train)\n",
      "score = clf.score(features_test, labels_test)\n",
      "print \"Score for Logistic Regression, default parameters (liblinear, l2, max_it=100,multi_class='ovr'): \", score\n",
      "\n",
      "estimator = linear_model.LogisticRegression(penalty='l1', solver='liblinear', multi_class='ovr')\n",
      "clf = estimator.fit(features_train, labels_train)\n",
      "score = clf.score(features_test, labels_test)\n",
      "print \"Score for Logistic Regression, penalty='l1', solver='liblinear', multi_class='ovr': \", score\n",
      "\n",
      "estimator = linear_model.LogisticRegression(penalty='l2', solver='lbfgs', multi_class='multinomial')\n",
      "clf = estimator.fit(features_train, labels_train)\n",
      "score = clf.score(features_test, labels_test)\n",
      "print \"Score for Logistic Regression, penalty='l2', solver='lbfgs', multi_class='multinomial': \", score\n",
      "\n",
      "estimator = linear_model.LogisticRegression(penalty='l2', solver='newton-cg', multi_class='multinomial')\n",
      "clf = estimator.fit(features_train, labels_train)\n",
      "score = clf.score(features_test, labels_test)\n",
      "print \"Score for Logistic Regression, penalty='l2', solver='newton-cg', multi_class='multinomial': \", score\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "estimator = linear_model.LogisticRegression(penalty='l2', solver='newton-cg', multi_class='multinomial', max_iter=200)\n",
      "clf = estimator.fit(features_train, labels_train)\n",
      "score = clf.score(features_test, labels_test)\n",
      "print \"Score for Logistic Regression, penalty='l2', solver='newton-cg', multi_class='multinomial', max_iter=200: \", score\n",
      "\n",
      "#print \"Using recursive feature elimination (RFE):\"\n",
      "\n",
      "for i in []:\n",
      "    # adding feature selection to this estimator, i features:\n",
      "    selector = RFE(estimator, i, step=1)\n",
      "    selector = selector.fit(features_train, labels_train)\n",
      "    score = selector.score(features_test, labels_test)\n",
      "    print \"Score for Logistic Regression with the \" + str(i) + \" most relevant features (RFE): \", score\n",
      "    print selector.get_support(indices=True)\n",
      "    \n",
      "# adding feature selection to this estimator:\n",
      "#selector = RFECV(estimator, step=1, cv=5)\n",
      "#selector = selector.fit(features_train, labels_train)\n",
      "#score = selector.score(features_test, labels_test)\n",
      "#print \"Score for Logistic Regression with the  most relevant features (RFECV): \", score\n",
      "#print selector.get_support(indices=True)\n",
      "\n",
      "print \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Score for Logistic Regression, no parameters:  0.657738095238\n",
        "Score for Logistic Regression, penalty='l1', solver='liblinear', multi_class='ovr': "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.669312169312\n",
        "Score for Logistic Regression, penalty='l2', solver='lbfgs', multi_class='multinomial': "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.479332010582\n",
        "Score for Logistic Regression, penalty='l2', solver='newton-cg', multi_class='multinomial': "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.701058201058\n",
        "Score for Logistic Regression, penalty='l2', solver='newton-cg', multi_class='multinomial': "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.702876984127\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Support Vector Classification\n",
      "estimator = svm.LinearSVC() # tune the parameters to outperform logistic regression\n",
      "clf = estimator.fit(features_train, labels_train)\n",
      "score = clf.score(features_test, labels_test)\n",
      "print \"Score for Support Vector Classification: \", score\n",
      "\n",
      "print \"Using recursive feature elimination (RFE):\"\n",
      "\n",
      "for i in []:\n",
      "    # adding feature selection to this estimator, i features:\n",
      "    selector = RFE(estimator, i, step=1)\n",
      "    selector = selector.fit(features_train, labels_train)\n",
      "    score = selector.score(features_test, labels_test)\n",
      "    print \"Score for Support Vector Classification with the \" + str(i) + \" most relevant features (RFE): \", score\n",
      "    print selector.get_support(indices=True)\n",
      "\n",
      "# adding feature selection to this estimator:\n",
      "selector = RFECV(estimator, step=1, cv=5)\n",
      "selector = selector.fit(features_train, labels_train)\n",
      "score = selector.score(features_test, labels_test)\n",
      "print \"Score for Support Vector Classification with the  most relevant features (RFECV): \", score\n",
      "print selector.get_support(indices=True)\n",
      "\n",
      "print \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Score for Support Vector Classification:  0.360945767196\n",
        "Using recursive feature elimination (RFE):\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-28-435068e8330b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# adding feature selection to this estimator:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mselector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRFECV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mselector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Score for Support Vector Classification with the  most relevant features (RFECV): \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/cesarpumar/.local/lib/python2.7/site-packages/sklearn/feature_selection/rfe.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    344\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mranking_\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m                 \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m                 \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/cesarpumar/.local/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    701\u001b[0m                                               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m                                               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m                                               rnd.randint(np.iinfo('i').max))\n\u001b[0m\u001b[0;32m    704\u001b[0m         \u001b[1;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m         \u001b[1;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Naive Bayes\n",
      "estimator = naive_bayes.GaussianNB() # tune the parameters to outperform logistic regression\n",
      "clf = estimator.fit(features_train, labels_train)\n",
      "score = clf.score(features_test, labels_test)\n",
      "print \"Score for Naive Bayes: \", score\n",
      "\n",
      "print \"Using recursive feature elimination (RFE):\"\n",
      "\n",
      "for i in []:\n",
      "    # adding feature selection to this estimator, i features:\n",
      "    selector = RFE(estimator, i, step=1)\n",
      "    selector = selector.fit(features_train, labels_train)\n",
      "    score = selector.score(features_test, labels_test)\n",
      "    print \"Score for Naive Bayes with the \" + str(i) + \" most relevant features (RFE): \", score\n",
      "\n",
      "# adding feature selection to this estimator:\n",
      "#selector = RFECV(estimator, step=1, cv=5)\n",
      "#selector = selector.fit(features_train, labels_train)\n",
      "#score = selector.score(features_test, labels_test)\n",
      "#print \"Score for Naive Bayes with the  most relevant features (RFECV): \", score\n",
      "\n",
      "print \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Score for Naive Bayes:  0.457010582011\n",
        "Using recursive feature elimination (RFE):\n"
       ]
      },
      {
       "ename": "AttributeError",
       "evalue": "'GaussianNB' object has no attribute 'coef_'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-8-9932e01a2157>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# adding feature selection to this estimator:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mselector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRFECV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mselector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Score for Naive Bayes with the  most relevant features (RFECV): \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/cesarpumar/.local/lib/python2.7/site-packages/sklearn/feature_selection/rfe.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m             \u001b[1;31m# Compute a full ranking of the features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m             \u001b[0mranking_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrfe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mranking_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m             \u001b[1;31m# Score each subset of features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mranking_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/cesarpumar/.local/lib/python2.7/site-packages/sklearn/feature_selection/rfe.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m                 \u001b[0mranks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_sqr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mAttributeError\u001b[0m: 'GaussianNB' object has no attribute 'coef_'"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Nearest Neighbors\n",
      "estimator = neighbors.KNeighborsClassifier() # tune the parameters to outperform logistic regression\n",
      "clf = estimator.fit(features_train, labels_train)\n",
      "score = clf.score(features_test, labels_test)\n",
      "print \"Score for Nearest Neighbors: \", score\n",
      "\n",
      "print \"Using recursive feature elimination (RFE):\"\n",
      "\n",
      "for i in []:\n",
      "    # adding feature selection to this estimator, i features:\n",
      "    selector = RFE(estimator, i, step=1)\n",
      "    selector = selector.fit(features_train, labels_train)\n",
      "    score = selector.score(features_test, labels_test)\n",
      "    print \"Score for Nearest Neighbors with the \" + str(i) + \" most relevant features (RFE): \", score\n",
      "\n",
      "# adding feature selection to this estimator:\n",
      "#selector = RFECV(estimator, step=1, cv=5)\n",
      "#selector = selector.fit(features_train, labels_train)\n",
      "#score = selector.score(features_test, labels_test)\n",
      "#print \"Score for Nearest Neighbors with the  most relevant features (RFECV): \", score\n",
      "\n",
      "print \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Score for Nearest Neighbors:  0.774636243386\n",
        "Using recursive feature elimination (RFE):\n"
       ]
      },
      {
       "ename": "AttributeError",
       "evalue": "'KNeighborsClassifier' object has no attribute 'coef_'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-10-8551eaffbdfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# adding feature selection to this estimator, i features:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mselector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRFE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mselector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Score for Nearest Neighbors with the \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" most relevant features (RFE): \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/cesarpumar/.local/lib/python2.7/site-packages/sklearn/feature_selection/rfe.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m                 \u001b[0mranks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_sqr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mAttributeError\u001b[0m: 'KNeighborsClassifier' object has no attribute 'coef_'"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Decision Tree\n",
      "estimator = tree.DecisionTreeClassifier() # tune the parameters to outperform logistic regression\n",
      "clf = estimator.fit(features_train, labels_train)\n",
      "score = clf.score(features_test, labels_test)\n",
      "print \"Score for Decision Tree: \", score\n",
      "\n",
      "print \"Using recursive feature elimination (RFE):\"\n",
      "\n",
      "for i in [5,10,15,20,25,30,35]:\n",
      "    # adding feature selection to this estimator, i features:\n",
      "    selector = RFE(estimator, i, step=1)\n",
      "    selector = selector.fit(features_train, labels_train)\n",
      "    score = selector.score(features_test, labels_test)\n",
      "    print \"Score for Decision Tree with the \" + str(i) + \" most relevant features (RFE): \", score\n",
      "\n",
      "# adding feature selection to this estimator:\n",
      "selector = RFECV(estimator, step=1, cv=5)\n",
      "selector = selector.fit(features_train, labels_train)\n",
      "score = selector.score(features_test, labels_test)\n",
      "print \"Score for Decision Tree with the  most relevant features (RFECV): \", score\n",
      "\n",
      "print \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Score for Decision Tree:  0.762235449735\n",
        "Using recursive feature elimination (RFE):\n"
       ]
      },
      {
       "ename": "AttributeError",
       "evalue": "'DecisionTreeClassifier' object has no attribute 'coef_'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-11-1de4e4dd7e45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# adding feature selection to this estimator, i features:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mselector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRFE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mselector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Score for Decision Tree with the \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" most relevant features (RFE): \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/cesarpumar/.local/lib/python2.7/site-packages/sklearn/feature_selection/rfe.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m                 \u001b[0mranks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_sqr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mAttributeError\u001b[0m: 'DecisionTreeClassifier' object has no attribute 'coef_'"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "######################\n",
      "### ENSEMBLE METHODS\n",
      "######################\n",
      "\n",
      "# AVERAGING\n",
      "###########\n",
      "\n",
      "# Random Forest\n",
      "estimator = RandomForestClassifier(n_estimators=10)\n",
      "clf = estimator.fit(features_train, labels_train)\n",
      "score = clf.score(features_test, labels_test)\n",
      "print \"Score for Random Forest: \", score\n",
      "\n",
      "print \"Using recursive feature elimination (RFE):\"\n",
      "\n",
      "for i in [5,10,15,20,25,30,35]:\n",
      "    # adding feature selection to this estimator, i features:\n",
      "    selector = RFE(estimator, i, step=1)\n",
      "    selector = selector.fit(features_train, labels_train)\n",
      "    score = selector.score(features_test, labels_test)\n",
      "    print \"Score for Random Forest with the \" + str(i) + \" most relevant features (RFE): \", score\n",
      "\n",
      "# adding feature selection to this estimator:\n",
      "selector = RFECV(estimator, step=1, cv=5)\n",
      "selector = selector.fit(features_train, labels_train)\n",
      "score = selector.score(features_test, labels_test)\n",
      "print \"Score for Random Forest with the  most relevant features (RFECV): \", score\n",
      "\n",
      "print \"\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Score for Random Forest:  0.811838624339\n",
        "Using recursive feature elimination (RFE):\n"
       ]
      },
      {
       "ename": "AttributeError",
       "evalue": "'RandomForestClassifier' object has no attribute 'coef_'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-12-7427cc46e86c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# adding feature selection to this estimator, i features:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mselector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRFE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mselector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Score for Random Forest with the \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" most relevant features (RFE): \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/cesarpumar/.local/lib/python2.7/site-packages/sklearn/feature_selection/rfe.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m                 \u001b[0mranks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msafe_sqr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mAttributeError\u001b[0m: 'RandomForestClassifier' object has no attribute 'coef_'"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# BOOSTING\n",
      "##########\n",
      "\n",
      "# AdaBoost\n",
      "clf = AdaBoostClassifier(n_estimators=10) # default = DecisionTreeClassifier. (base_estimator=tree.DecisionTreeClassifier, )\n",
      "scores = cross_val_score(clf, features, labels)\n",
      "print \"Score for AdaBoost: \", scores.mean()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Score for AdaBoost:  0.412037037037\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#############################\n",
      "## UNCOMMENT FOR SUBMISSION: \n",
      "##############################################################\n",
      "### LEARNING FROM DATA WITH THE ENTIRE DATASET ###############\n",
      "##############################################################\n",
      "\n",
      "\n",
      "# learner = linear_model.LogisticRegression() # uncomment when preparing\n",
      "# #learner = svm.LinearSVC() # for submission\n",
      "# #learner = RandomForestClassifier(n_estimators=10)\n",
      "# learner.fit(features,labels) \n",
      "\n",
      "# # [TEST] if learning works with Logistic Regression\n",
      "# # predicted_class_LogReg = learnerLogReg.predict([2596,51,3,258,0,510,221,232,148,6279,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0])\n",
      "# # print predicted_class_LogReg\n",
      "\n",
      "# # Test if learning works with Support Vector Machines\n",
      "# # predicted_class_SVC = learner.predict([2596,51,3,258,0,510,221,232,148,6279,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0])\n",
      "# # print predicted_class_SVC\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#############################\n",
      "## UNCOMMENT FOR SUBMISSION: \n",
      "# #######################################\n",
      "# ### CREATE A CSV FILE FOR THE RESULTS\n",
      "# #######################################\n",
      "\n",
      "\n",
      "# features_test = load_csv_test('test.csv',',')\n",
      "\n",
      "# # SCORE the model\n",
      "# # predict and write in the csv\n",
      "\n",
      "# output.write(\"Id,Cover_Type\\n\")\n",
      "\n",
      "# id_data = 15121\n",
      "# for datapoint in features_test:\n",
      "\n",
      "# \t# predict the cover type\n",
      "# \tpredicted_label = learner.predict(datapoint)\n",
      "\n",
      "# \t# write id and cover type\n",
      "# \toutput.write(str(id_data)+\",\"+str(predicted_label[0])+\"\\n\")\n",
      "\n",
      "# \tid_data += 1\n",
      "\n",
      "\n",
      "# # [TEST] print into a file the result of predictions:\n",
      "# counter=0\n",
      "# # .. TO-DO ..\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "###### FILES ######################################\n",
      "# #close output\n",
      "output.close()\n",
      "##################################################"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    }
   ],
   "metadata": {}
  }
 ]
}