{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CHANGE WORKING DIRECTORY AND ADD MY LIBRARIES\n",
    "import os\n",
    "os.chdir('./4.Santander_Customer_Satisfaction')\n",
    "import sys\n",
    "sys.path.insert(0, '../mylib/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYSPARK SETTINGS\n",
    "sys.path.append(\"C:\\spark\\python\\lib\\pyspark.zip\")\n",
    "sys.path.append(\"C:\\spark\\python\\lib\\py4j-0.9-src.zip\")\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"C:\\spark\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\hadoop\" \n",
    "# see http://nishutayaltech.blogspot.co.uk/2015/04/how-to-run-apache-spark-on-windows7-in.html for more info\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGES\n",
    "\n",
    "from pyspark.sql.functions import stddev\n",
    "from pyspark.sql.functions import min\n",
    "from pyspark.sql.functions import udf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.mllib.classification import SVMWithSGD, SVMModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAGIC COMMANDS\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test:\n",
    "lines = sc.textFile('README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA FILES (CSV)\n",
    "\n",
    "training_df = sqlContext.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(inferSchema=\"true\", header='true') \\\n",
    "    .load('train.csv')\n",
    "\n",
    "\n",
    "test_df = sqlContext.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(inferSchema=\"true\", header='true') \\\n",
    "    .load('test.csv')\n",
    "\n",
    "\n",
    "sample_submission_df = sqlContext.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .options(inferSchema=\"true\", header='true') \\\n",
    "    .load('sample_submission.csv')\n",
    "\n",
    "# See http://spark.apache.org/docs/latest/sql-programming-guide.html for examples on how to deal with Spark SQL dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: show target\n",
    "#training_df.select('TARGET').show(30) # (returns another dataframe and shows it)\n",
    "# which is not the same as training_df['var15'] (returns a Column object, the equivalent to Series?)\n",
    "# have a look here: http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE: show age and target. add 1 unit to age. This is also an example of operations over columns\n",
    "#training_df.select(training_df['var15'] + 1, training_df['TARGET']).show(50)\n",
    "# example: \n",
    "#type(training_df.select('var15', 'TARGET'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of the dataset as a pandas dataframe, \n",
    "# so that dealing with this small amount of information (in comparison with the dataset) is much quicker.\n",
    "pandas_df_training_describe = training_df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out which object has a std() method\n",
    "\n",
    "#type(training_df['var15']) # Column -> stddev\n",
    "#type(training_df.var15) # Column\n",
    "#type(training_df.select('var15')) # DataFrame\n",
    "\n",
    "# This was confusing at the beginning. A Column is a name or an expression: stddev('var15') to be used as selector. \n",
    "# It does not contain data (check this statement later). \n",
    "# See https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove constant features\n",
    "\n",
    "# Get std of all variables (row 2, starting from column 1) as a dict.\n",
    "training_stds = pandas_df_training_describe.iloc[2,1:].apply(float).to_dict()\n",
    "\n",
    "# get those columns whose std == 0.0\n",
    "remove = [col for col, value in training_stds.items() if value == 0.0]\n",
    "\n",
    "for col in remove:\n",
    "    training_df = training_df.drop(col)\n",
    "    test_df = test_df.drop(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated columns: \n",
    "# THIS CODE IS TOO SLOW. I NEED TO FIND AN ALTERNATIVE WAY\n",
    "remove = []\n",
    "\n",
    "# Get col names from the describe pandas dataframe:\n",
    "cols = pandas_df_training_describe.columns[1:]  # all columns but the first one, which in this dataframe is \"summary\"\n",
    "\n",
    "# from the first to the penultimate column\n",
    "for i in range(0, len(cols)-1):\n",
    "    for j in range(i+1, len(cols)): # compare with the following columns but using the correlation coefficient. If == 1, equal cols.\n",
    "        if training_df.corr(cols[i], cols[j]) == 1:\n",
    "            remove.append(cols[j])\n",
    "\n",
    "remove\n",
    "\n",
    "#for i in range(len(c)-1):\n",
    "#    v = training_df[c[i]].values\n",
    "#    for j in range(i+1,len(c)):\n",
    "#        if np.array_equal(v,training_df[c[j]].values):\n",
    "#            remove.append(c[j])\n",
    "            \n",
    "#training_df.drop(remove, axis=1, inplace=True)\n",
    "#test_df.drop(remove, axis=1, inplace=True)\n",
    "\n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|max(count)|\n+----------+\n|         1|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# EXPLORATORY ANALYSIS:\n",
    "# check if there is any repeated ID, which would imply to tidy the data set:\n",
    "#id_counts_df = training_df['ID'].value_counts().sort_index()  # count the number of occurrences of each ID\n",
    "#max(id_counts_df)  # if the max value is 1, then there are no repeated IDs = 1 row for each observation.\n",
    "\n",
    "\n",
    "id_counts_df = training_df.groupBy('ID').count()\n",
    "id_counts_df.groupBy().max('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the describe table to a file\n",
    "describe_file = open('describe_file_pyspark.txt', 'w+')\n",
    "describe_file.write(pandas_df_training_describe.to_json()) \n",
    "describe_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76020, 337)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore variables graphically with pyspark. Two options came to my mind: \n",
    "#   1. get a small subset of the dataset and do traditional visualization with pandas and seaborn.\n",
    "#   2. try Apache Zeppeling, which is still incubating\n",
    "\n",
    "# Let's go for the firs one. In order to know the fraction of the training set we want to obtain, \n",
    "# it would be useful to know its size first:\n",
    "\n",
    "# Get column names\n",
    "column_names = training_df.columns  # show all column names. Remember 1st element is \"Summary\"\n",
    "#column_names\n",
    "\n",
    "training_df_shape = (training_df.count(), len(column_names))\n",
    "training_df_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3842, 337)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Once we know the size, lets get a fraction of the dataframe and then convert it to pandas dataframe:\n",
    "subset_training_df = training_df.sample(False,0.05).toPandas()\n",
    "subset_training_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>var3</th>\n",
       "      <th>var15</th>\n",
       "      <th>imp_ent_var16_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult3</th>\n",
       "      <th>imp_op_var40_comer_ult1</th>\n",
       "      <th>imp_op_var40_comer_ult3</th>\n",
       "      <th>imp_op_var40_efect_ult1</th>\n",
       "      <th>imp_op_var40_efect_ult3</th>\n",
       "      <th>...</th>\n",
       "      <th>saldo_medio_var33_hace2</th>\n",
       "      <th>saldo_medio_var33_hace3</th>\n",
       "      <th>saldo_medio_var33_ult1</th>\n",
       "      <th>saldo_medio_var33_ult3</th>\n",
       "      <th>saldo_medio_var44_hace2</th>\n",
       "      <th>saldo_medio_var44_hace3</th>\n",
       "      <th>saldo_medio_var44_ult1</th>\n",
       "      <th>saldo_medio_var44_ult3</th>\n",
       "      <th>var38</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>76828.980739</td>\n",
       "      <td>-2339.789953</td>\n",
       "      <td>33.164237</td>\n",
       "      <td>115.328769</td>\n",
       "      <td>78.298493</td>\n",
       "      <td>127.008155</td>\n",
       "      <td>5.725315</td>\n",
       "      <td>9.836760</td>\n",
       "      <td>0.167694</td>\n",
       "      <td>0.167694</td>\n",
       "      <td>...</td>\n",
       "      <td>32.771611</td>\n",
       "      <td>0.373345</td>\n",
       "      <td>45.850409</td>\n",
       "      <td>30.042283</td>\n",
       "      <td>12.673571</td>\n",
       "      <td>1.901627</td>\n",
       "      <td>38.082033</td>\n",
       "      <td>36.247137</td>\n",
       "      <td>122351.268509</td>\n",
       "      <td>0.041645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>43866.243355</td>\n",
       "      <td>48349.345212</td>\n",
       "      <td>12.698980</td>\n",
       "      <td>2284.990662</td>\n",
       "      <td>358.626038</td>\n",
       "      <td>531.096111</td>\n",
       "      <td>114.384073</td>\n",
       "      <td>191.324687</td>\n",
       "      <td>6.619606</td>\n",
       "      <td>6.619606</td>\n",
       "      <td>...</td>\n",
       "      <td>1035.295994</td>\n",
       "      <td>19.546962</td>\n",
       "      <td>1472.095905</td>\n",
       "      <td>969.884422</td>\n",
       "      <td>686.905932</td>\n",
       "      <td>117.870184</td>\n",
       "      <td>1131.059386</td>\n",
       "      <td>1111.408040</td>\n",
       "      <td>278975.937365</td>\n",
       "      <td>0.199803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>125.000000</td>\n",
       "      <td>-999999.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12088.950000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>38752.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>69137.842500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>77705.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>105098.835000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>113933.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>117310.979016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>151767.000000</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>129000.000000</td>\n",
       "      <td>9946.710000</td>\n",
       "      <td>11140.080000</td>\n",
       "      <td>5122.290000</td>\n",
       "      <td>7740.780000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>38838.390000</td>\n",
       "      <td>1200.030000</td>\n",
       "      <td>63317.190000</td>\n",
       "      <td>42767.160000</td>\n",
       "      <td>42316.440000</td>\n",
       "      <td>7306.050000</td>\n",
       "      <td>50415.360000</td>\n",
       "      <td>50415.360000</td>\n",
       "      <td>11857856.460000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 337 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>var3</th>\n",
       "      <th>var15</th>\n",
       "      <th>imp_ent_var16_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult1</th>\n",
       "      <th>imp_op_var39_comer_ult3</th>\n",
       "      <th>imp_op_var40_comer_ult1</th>\n",
       "      <th>imp_op_var40_comer_ult3</th>\n",
       "      <th>imp_op_var40_efect_ult1</th>\n",
       "      <th>imp_op_var40_efect_ult3</th>\n",
       "      <th>...</th>\n",
       "      <th>saldo_medio_var33_hace2</th>\n",
       "      <th>saldo_medio_var33_hace3</th>\n",
       "      <th>saldo_medio_var33_ult1</th>\n",
       "      <th>saldo_medio_var33_ult3</th>\n",
       "      <th>saldo_medio_var44_hace2</th>\n",
       "      <th>saldo_medio_var44_hace3</th>\n",
       "      <th>saldo_medio_var44_ult1</th>\n",
       "      <th>saldo_medio_var44_ult3</th>\n",
       "      <th>var38</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "      <td>3842.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>76828.980739</td>\n",
       "      <td>-2339.789953</td>\n",
       "      <td>33.164237</td>\n",
       "      <td>115.328769</td>\n",
       "      <td>78.298493</td>\n",
       "      <td>127.008155</td>\n",
       "      <td>5.725315</td>\n",
       "      <td>9.836760</td>\n",
       "      <td>0.167694</td>\n",
       "      <td>0.167694</td>\n",
       "      <td>...</td>\n",
       "      <td>32.771611</td>\n",
       "      <td>0.373345</td>\n",
       "      <td>45.850409</td>\n",
       "      <td>30.042283</td>\n",
       "      <td>12.673571</td>\n",
       "      <td>1.901627</td>\n",
       "      <td>38.082033</td>\n",
       "      <td>36.247137</td>\n",
       "      <td>122351.268509</td>\n",
       "      <td>0.041645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>43866.243355</td>\n",
       "      <td>48349.345212</td>\n",
       "      <td>12.698980</td>\n",
       "      <td>2284.990662</td>\n",
       "      <td>358.626038</td>\n",
       "      <td>531.096111</td>\n",
       "      <td>114.384073</td>\n",
       "      <td>191.324687</td>\n",
       "      <td>6.619606</td>\n",
       "      <td>6.619606</td>\n",
       "      <td>...</td>\n",
       "      <td>1035.295994</td>\n",
       "      <td>19.546962</td>\n",
       "      <td>1472.095905</td>\n",
       "      <td>969.884422</td>\n",
       "      <td>686.905932</td>\n",
       "      <td>117.870184</td>\n",
       "      <td>1131.059386</td>\n",
       "      <td>1111.408040</td>\n",
       "      <td>278975.937365</td>\n",
       "      <td>0.199803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>125.000000</td>\n",
       "      <td>-999999.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12088.950000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>38752.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>69137.842500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>77705.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>105098.835000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>113933.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>117310.979016</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>151767.000000</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>129000.000000</td>\n",
       "      <td>9946.710000</td>\n",
       "      <td>11140.080000</td>\n",
       "      <td>5122.290000</td>\n",
       "      <td>7740.780000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>38838.390000</td>\n",
       "      <td>1200.030000</td>\n",
       "      <td>63317.190000</td>\n",
       "      <td>42767.160000</td>\n",
       "      <td>42316.440000</td>\n",
       "      <td>7306.050000</td>\n",
       "      <td>50415.360000</td>\n",
       "      <td>50415.360000</td>\n",
       "      <td>11857856.460000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 337 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe the subset:\n",
    "subset_training_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the distribution of var15 graphically:\n",
    "sns.set(rc={\"figure.figsize\": (16, 8)})\n",
    "plt = sns.distplot(subset_training_df['var15'], \n",
    "             hist_kws={\"linewidth\": 1},  # histogram\n",
    "             rug_kws={\"color\": \"g\"},  # Plot datapoints in an array as sticks on an axis.\n",
    "             kde_kws={\"color\": \"b\", \"lw\": 2, \"label\": \"mean\"}  # Fit and plot a univariate or bivariate kernel density estimate.\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The idea of exploring a subset graphically can be extended to other steps. I just thought about the words \"Subset Engineering\", \n",
    "# which could be a new step used when dealing with big data sets which have to be reduced to easily deal with them. This step \n",
    "# could be based on several heuristics to create meaninful subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are several boolean variables in the dataset. Lets figure out what they mean. \n",
    "# First, I will get all their names and the proportion of 0s and 1s\n",
    "\n",
    "# THIS CODE IS TOO SLOW. I have to find an alternative\n",
    "\n",
    "#print(training_df.select('ID').collect(0))\n",
    "\n",
    "#for col in column_names:\n",
    "#    print training_df.select(min(col)).collect()[0][0]\n",
    "\n",
    "#booleans = [col for col in column_names \n",
    "#            if training_df.select(min(col)).collect()[0][0] == 0 \n",
    "#            if training_df.select(max(col)).collect()[0][0] == 1]\n",
    "\n",
    "#list(training_df[booleans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA TRANSFORMATION\n",
    "\n",
    "# remove the ID field from DataFrames, but save first\n",
    "\n",
    "training_IDs = training_df.select('ID')\n",
    "test_IDs = test_df.select('ID')\n",
    "\n",
    "# drop() does not modify the current dataframe. It returns a new dataframe with the chosen column removed\n",
    "training_df = training_df.drop('ID') \n",
    "test_df = test_df.drop('ID')\n",
    "\n",
    "# update column names:\n",
    "column_names = training_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform age column to: likely to change = 1, unlikely to change = 0. Lets establish the threshold above 40 years old.\n",
    "\n",
    "# - Following http://stackoverflow.com/questions/29109916/updating-a-dataframe-column-in-spark, there are several alternatives, \n",
    "# both at DataFrame and RDD level\n",
    "# - Useful tips when transitioning from pandas dataframe to spark dataframe\n",
    "# http://growthintel.com/from-pandas-to-spark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame select/udf - based transformation:\n",
    "def var15_tobool_f(var):\n",
    "    if var < 40:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "var15_tobool = udf(var15_tobool_f)\n",
    "index_var15 = training_df.columns.index('var15')\n",
    "\n",
    "column_names = training_df.columns  # save current column names\n",
    "training_df = training_df.select(column_names[:index_var15]+[var15_tobool('var15')]+column_names[index_var15+1:])\n",
    "training_df = training_df.withColumnRenamed(training_df.columns[index_var15], 'var15')\n",
    "# Given the name 'var15' changes to 'PythonUDF#var15_tobool_f(var15)', this column name has to be restored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Notes on RDDs:\n",
    "\n",
    "# why does a row of an rdd admit operations like x['var15'] or x.var15? (see the previous lambda x func)\n",
    "# rdds coming from a dataframe contain Row objects, which contain the schema from the original dataframe\n",
    "# Check it by doing: \n",
    "#rdd_t = training_df.rdd.map(lambda x: type(x))\n",
    "#rdd_t.first()\n",
    "\n",
    "# By checking the documentation, this can be confirmed:\n",
    "# DataFrame.rdd: Returns the content as an pyspark.RDD of Row.\n",
    "# Row: A row in DataFrame. The fields in it can be accessed like attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection (mllib) the only one available in mllib is ChiSqSelector, which is not still available for pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFIER & TRAINING: support vector machine, class imbalance handling\n",
    "\n",
    "# Load and parse the data\n",
    "#def prepare(row):\n",
    "#    return LabeledPoint(row[-1], row[:-1])\n",
    "\n",
    "#training_df.map(prepare) and training_df.map(lambda row: prepare(row)) are equivalent\n",
    "#prepared_training_df = training_df.map(prepare)\n",
    "\n",
    "# A compact version of the previous lines:\n",
    "prepared_training_df = training_df.map(lambda row: LabeledPoint(row[-1], row[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = SVMWithSGD.train(prepared_training_df, iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and load model\n",
    "model.save(sc, \"MLlib_SVM_model\")\n",
    "reloadedModel = SVMModel.load(sc, \"MLlib_SVM_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.0395685345961589\n"
     ]
    }
   ],
   "source": [
    "# TEST: prepare test RDD\n",
    "#prepared_training_df.first().label\n",
    "#prepared_training_df.first().features\n",
    "\n",
    "# Create an RDD of tuples (true label, predicted label) # float() is important!\n",
    "predictionAndLabels = prepared_training_df.map(lambda row: (float(model.predict(row.features)), row.label))\n",
    "\n",
    "# create an RDD of tuples where true label != predicted label, count the amount and divide by the total number of tuples\n",
    "trainErr = predictionAndLabels.filter(lambda row: row[0] != row[1]).count() / float(predictionAndLabels.count()) \n",
    "\n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metrics:\n",
    "metrics = BinaryClassificationMetrics(predictionAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR = 0.5197842672980795\nArea under ROC = 0.5\n"
     ]
    }
   ],
   "source": [
    "# Area under precision-recall curve\n",
    "print(\"Area under PR = %s\" % metrics.areaUnderPR)\n",
    "\n",
    "# Area under ROC curve\n",
    "print(\"Area under ROC = %s\" % metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\ml\\classification.py:207: UserWarning: weights is deprecated. Use coefficients instead.\n"
     ]
    }
   ],
   "source": [
    "# Cross - validation:\n",
    "# see http://spark.apache.org/docs/latest/ml-guide.html (overview of MLlib: estimators, transformers and pipelines)\n",
    "# and then https://spark.apache.org/docs/1.6.1/api/python/pyspark.ml.html#module-pyspark.ml.tuning\n",
    "# ml is different from mllib, keep it in mind and compare both. \n",
    "# Remember I previously did the following with mllib: \n",
    "#   model = SVMWithSGD.train(prepared_training_df, iterations=5) \n",
    "#   predictionAndLabels = prepared_training_df.map(lambda row: (float(model.predict(row.features)), row.label)) \n",
    "# ml module uses fit() and transform() instead:\n",
    "\n",
    "# Create a LogisticRegression instance. This instance is an Estimator.\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "\n",
    "# Learn a LogisticRegression model. I use prepared_training_df, \n",
    "# as ml also uses the convention (label, features) instead of (features, label) to learn from the training data.\n",
    "# Estimator.fit() uses a dataframe as input an returns a Transformer model as output.\n",
    "# prepared_training_df is an RDD, which is ok for mllib package, but not for ml, which requires dataframes to be used.\n",
    "# So we have to turn the prepared_training_df RDD into a dataframe again.\n",
    "prepared_training_df_ml = prepared_training_df.toDF()\n",
    "\n",
    "# train the model\n",
    "model = lr.fit(prepared_training_df_ml)  \n",
    "\n",
    "# Make predictions on test data using the Transformer.transform() method.\n",
    "# Using the training data as test data is not a good approach. Anyways, the purpose of this code block is to understand \n",
    "# the ml package. I will go back to this problem later.\n",
    "prediction = model.transform(prepared_training_df_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=DenseVector([2.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 3.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 99.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39205.17]), label=0.0, rawPrediction=DenseVector([2.5272, -2.5272]), probability=DenseVector([0.926, 0.074]), prediction=0.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}